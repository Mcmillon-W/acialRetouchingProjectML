# -*- coding: utf-8 -*-
"""SRBM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bhI_tTmj0Wf6kGxJJ3aJ25sL4mG1WAJJ
"""

import os
os.chdir('/content/drive/My Drive/BTP/codes/')

import numpy as np
import matplotlib.pyplot as plt
import random
from sklearn.model_selection import train_test_split
import cv2
import facial_feature_extraction
import glob
import progressbar


def forward_pass(data_points,W,V,Y) :
    Z = compute_Z_values(W, data_points) # n * (H+1)
    output_matrix = compute_O_values(V, Z) # n * K
    O_softmax = compute_softmax(output_matrix) # n * K
    error = calculate_error(O_softmax, Y, 1e-10)
    return error,O_softmax

def calculate_error(predictions, targets, epsilon=1e-10):
    '''
    pred = epsilon*np.ones(predictions.shape)
    predictions = predictions + pred # to avoid any of element in predictions to be 0
    res = targets * np.log(predictions) # n*K
    total_cost = (-1) * np.sum(res, axis=1) # n*1
    cross_entropy_error = np.mean(total_cost) #1*1
    return cross_entropy_error
    '''
    error = 0
    for i in range(targets.shape[0]):
      if(not( (predictions[i][0]>predictions[i][1]) ^ (targets[i][0]>targets[i][1]) )): ## Both true or both false
        error += 1
    return (error/targets.shape[0])

def compute_softmax(matrix) :
    e = np.exp(matrix)
    matrix = e / e.sum(axis=1, keepdims=True)
    return matrix

def compute_Z_values(weights,data_points) :
    z_values = np.dot(data_points, weights.T) # n * H
    z_values = np.tanh(z_values) # n * H
    z_values = np.column_stack((np.ones((len(z_values), 1)), z_values)) # n * (H+1)
    return z_values # n * (H+1)

def compute_O_values(weights,z_values) :
    o_values = np.dot(weights, z_values.T) # K * n
    return o_values.T # n * K

def initilaise_weights(data) :
    final_data = np.column_stack((np.ones((len(data), 1)), data))
    return final_data # N * (D+1)

def random_weights(number_of_rows,number_of_columns) :
    new_data = np.random.normal(0,1,(number_of_rows, number_of_columns))
    return new_data

def backward_pass(O_softmax,Y,V,Z,W,X,bias_z):
    '''
    O_softmax <= n*K, Y <= n*K, V <= K*(H+1) 
    Z <= n*(H+1), W <= H*(D+1), X <= n*(D+1)
    bias_z <= n*1
    '''
    W_new = gradient_input_to_hidden(O_softmax,Y,V,Z,X) # H*(D+1)
    V_new, bias_v = gradient_hidden_to_output(O_softmax,Y,Z,bias_z) # K*H, K*1
    return  W_new, V_new, bias_v

def gradient_hidden_to_output(O_softmax,Y,Z,bias_z) :
    Zbar = Z[:,1:] # n*H
    final_result_matrix = (np.dot((O_softmax - Y).T, Zbar)) # K * H
    bias_v = np.dot((O_softmax).T, bias_z) # K * 1
    return final_result_matrix,bias_v


def gradient_input_to_hidden(O_softmax,Y,V,Z,X) :
    B, H = Z.shape
    H = H-1
    H_B_1 = np.ones((H, B))
    Zbar = Z[:,1:] # H*n
    Zbar = Zbar.T # n*H
    Jacobian = H_B_1 - (Zbar*Zbar) # H*n
    delta3 = (O_softmax - Y).T # K*n
    Vbar = V[:,1:] # K*H
    del2 = np.dot(Vbar.T, delta3) # H*n
    delta2 = del2 * Jacobian # H*n
    result_matrix = np.dot(delta2, X) # H*(D+1)
    return result_matrix

def shuffle(a, b, seed):
   random.seed(seed)
   c = list(zip(a, b))
   random.shuffle(c)
   a, b = zip(*c)
   b = np.stack(b, axis=0)
   a = np.stack(a, axis=0)
   return a,b

def train_util(data,labels,srm_no):
    print("SRBM no. " + srm_no + " out of total 7 SRMs of 2 layer each training start...")
    #data : N * D
    #Y : N * K
    #labels : N * 1
    # note : K = 2

    Y = np.zeros((labels.shape[0],2))
    for i in range(labels.shape[0]):
      if(labels[i]==0):
        Y[i]=[1,0]
      else:
        Y[i]=[0,1]

    X = initilaise_weights(data) # N * (D+1)

    N,D = data.shape
    N,K = Y.shape 
    H = D//100 # number of hidden units

    batch_size = 1
    learning_rate = 0.01
    train_validation_split = 0.2
    number_of_epocs = 100
    i=0

    bias_z = np.empty(shape=(batch_size, 1)) # 25 * 1
    bias_z.fill(1.0)
    
    train_data_x,validation_data_x,train_data_y,validation_data_y = train_test_split(X, Y, test_size=train_validation_split, random_state=42)

    train_data_len = len(train_data_x)
    validation_data_len = len(validation_data_x)

    min_validation_error = 1000000
    W = random_weights(H,D+1)
    V = random_weights(K,H+1)
    save_W,save_V = W,V

    #print("Started calculating Training error in 3 Trails for each epoch with Batch size =  ", batch_size)
    #print("Started calculating Validation error in 3 Trails for each epoch with Batch size =  ", batch_size)

    with progressbar.ProgressBar(max_value=3*number_of_epocs) as bar:
      for k in range(3) :
          W = random_weights(H,D+1)
          V = random_weights(K,H+1)
          error_train = 0
          error_validation = 0
          seed = random.randint(10000,10000000)
          X,Y = shuffle(X,Y,seed)

          #print("Training Error for 100th epoch for Trail Number : " + str(k+1))
          
          for j in range(number_of_epocs):
              i=0
              count=0
              error_train = 0.0
              error_validation = 0.0
              while i < (train_data_len)  :
                  i1=i
                  i= i+batch_size
                  peek1 = X[i1:i, :]
                  peek2 = Y[i1:i, :]
                  error,O_softmax = forward_pass(peek1, W, V, peek2) # 1*1, 1*K
                  Z = compute_Z_values(W, peek1)
                  W_new, V_new, bias_v = backward_pass(O_softmax,peek2,V,Z,W,peek1,bias_z)
                  W = W - (learning_rate/batch_size)*W_new
                  V_new = np.append(learning_rate*bias_v,V_new,axis=1)
                  V = V - (learning_rate/batch_size)*V_new
                  error_train += error
                  count += 1
              lengt = validation_data_len + train_data_len
              error, O_softmax = forward_pass(X[0:train_data_len, :], W, V, Y[0 : train_data_len, :])
              error1, O_softmax = forward_pass(X[train_data_len:lengt, :], W, V, Y[train_data_len:lengt, :])
              error_validation = error1
              count = train_data_len/batch_size
              count1= validation_data_len/batch_size
              error_train = error_train/count
              #print("Training error after  epoch : " + str(j+1) + " here batch size = " + str(batch_size))
              #print(error_train)
              #print("Validation error after  epoch : " + str(j+1) + " here batch size = " + str(batch_size))
              #print(error_validation)
              if(min_validation_error > error_validation):
                min_validation_error = error_validation
                save_W = W
                save_V = V
              bar.update(number_of_epocs*k + j)

    print("done!!!")
    print("validation_error : ", min_validation_error)
    return save_W

def train(path1,path2,total_original,total_retouched,labels):
  # fp : face_patch
  print("Extracting facial features from images(original)")
  fp10,fp20,fp30,fp40,fp50,fp60,fp70 = facial_feature_extraction.read_data(path1,total_original)
  print("Extracting facial features from images(retouched)")
  fp11,fp21,fp31,fp41,fp51,fp61,fp71 = facial_feature_extraction.read_data(path2,total_retouched)
  fp1 = np.concatenate((fp10,fp11),axis=0) # n*D1
  fp2 = np.concatenate((fp20,fp21),axis=0) # n*D2
  fp3 = np.concatenate((fp30,fp31),axis=0) # n*D3
  fp4 = np.concatenate((fp40,fp41),axis=0) # n*D4
  fp5 = np.concatenate((fp50,fp51),axis=0) # n*D5
  fp6 = np.concatenate((fp60,fp61),axis=0) # n*D6
  fp7 = np.concatenate((fp70,fp71),axis=0) # n*D7

  W1 = train_util(fp1,labels,'1_layer1')
  W2 = train_util(fp2,labels,'2_layer1')
  W3 = train_util(fp3,labels,'3_layer1')
  W4 = train_util(fp4,labels,'4_layer1')
  W5 = train_util(fp5,labels,'5_layer1')
  W6 = train_util(fp6,labels,'6_layer1')
  W7 = train_util(fp7,labels,'7_layer1')

  a1 = compute_Z_values(W1,initilaise_weights(fp1)) # n * (D1/16+1)
  #a1 = compute_Z_values(train_util(a1[:,1:],labels,'1_layer2'),a01) # n * (D1/64+1)
  a2 = compute_Z_values(W2,initilaise_weights(fp2)) # n * (D2/16+1)
  #a2 = compute_Z_values(train_util(a2[:,1:],labels,'2_layer2'),a02) # n * (D2/64+1)
  a3 = compute_Z_values(W3,initilaise_weights(fp3)) # n * ((D3/16+1)
  #a3 = compute_Z_values(train_util(a3[:,1:],labels,'3_layer2'),a03) # n * (D3/64+1)
  a4 = compute_Z_values(W4,initilaise_weights(fp4)) # n * (D4/16+1)
  #a4 = compute_Z_values(train_util(a4[:,1:],labels,'4_layer2'),a04) # n * (D4/64+1)
  a5 = compute_Z_values(W5,initilaise_weights(fp5)) # n * (D5/16+1)
  #a5 = compute_Z_values(train_util(a5[:,1:],labels,'5_layer2'),a05) # n * (D5/64+1)
  a6 = compute_Z_values(W6,initilaise_weights(fp6)) # n * (D6/16+1)
  #a6 = compute_Z_values(train_util(a6[:,1:],labels,'6_layer2'),a06) # n * (D6/64+1)
  a7 = compute_Z_values(W7,initilaise_weights(fp7)) # n * (D7/16+1)
  #a7 = compute_Z_values(train_util(a7[:,1:],labels,'7_layer2'),a07) # n * (D7/64+1)  

  concat_results = np.concatenate((a1[:,1:],a2[:,1:],a3[:,1:],a4[:,1:],a5[:,1:],a6[:,1:],a7[:,1:]),axis=1)
  #concat_results = np.concatenate((fp1,fp2,fp3,fp4,fp5,fp6,fp7),axis=1)
  
  input_to_SVM = concat_results # n*(7H)
  return input_to_SVM,W1,W2,W3,W4,W5,W6,W7

def output_SRBMS(path1, path2,total_original,total_retouched):
  label_to_SVM = np.concatenate((np.zeros((total_original,1)),np.ones((total_retouched,1))), axis=0) # n*1 
  input_to_SVM,W1,W2,W3,W4,W5,W6,W7 = train(path1, path2, total_original, total_retouched,label_to_SVM) #n*D
  print("total_samples: ", input_to_SVM.shape[0])
  print("feature_per_sample:", input_to_SVM.shape[1])
  return input_to_SVM, label_to_SVM,W1,W2,W3,W4,W5,W6,W7

def test(path1,path2,total_original,total_retouched):
  label_to_SVM = np.concatenate((np.zeros((total_original,1)),np.ones((total_retouched,1))), axis=0) # n*1 
  # fp : face_patch
  print("Extracting facial features from images(original)")
  fp10,fp20,fp30,fp40,fp50,fp60,fp70 = facial_feature_extraction.read_data(path1,total_original)
  print("Extracting facial features from images(retouched)")
  fp11,fp21,fp31,fp41,fp51,fp61,fp71 = facial_feature_extraction.read_data(path2,total_retouched)
  fp1 = np.concatenate((fp10,fp11),axis=0) # n*D1
  fp2 = np.concatenate((fp20,fp21),axis=0) # n*D2
  fp3 = np.concatenate((fp30,fp31),axis=0) # n*D3
  fp4 = np.concatenate((fp40,fp41),axis=0) # n*D4
  fp5 = np.concatenate((fp50,fp51),axis=0) # n*D5
  fp6 = np.concatenate((fp60,fp61),axis=0) # n*D6
  fp7 = np.concatenate((fp70,fp71),axis=0) # n*D7
  
  return fp1,fp2,fp3,fp4,fp5,fp6,fp7,label_to_SVM


